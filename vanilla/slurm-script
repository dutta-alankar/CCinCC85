#!/bin/bash -l

# Standard output and error:
#SBATCH -o ./tjob.%x.out.%j
#SBATCH -e ./tjob.%x.err.%j
#SBATCH --job-name="OldBoy"
#
# Number of nodes and MPI tasks per node:
#SBATCH --nodes=6
#SBATCH --ntasks-per-node=40
#
#SBATCH --mail-type=none
#SBATCH --mail-user=alankard@mpa-garching.mpg.de
#
# Partition
#SBATCH --partition=p.24h
# Wall clock limit:
#SBATCH --time=00-23:58:00

# Load compiler and MPI modules with explicit version specifications,
# consistently with the versions used to build the executable.


echo "Working Directory = $(pwd)"

cd $SLURM_SUBMIT_DIR
export PROG="./pluto"  # -catalyst 1 AllFieldsCatalyst.py"
mkdir -p $SLURM_SUBMIT_DIR/output/Log_Files

module purge
module load gcc/10 openmpi/4 hdf5-mpi/1.12.0

export LD_LIBRARY_PATH="/mpcdf/soft/SLE_15/packages/skylake/openmpi/gcc_10-10.3.0/4.0.7/lib:/mpcdf/soft/SLE_15/packages/skylake/gsl/gcc_10-10.3.0/2.4/lib:/mpcdf/soft/SLE_15/packages/skylake/fftw/gcc_10-10.3.0-openmpi_4-4.0.7/3.3.10/lib:/mpcdf/soft/SLE_15/packages/skylake/hdf5/gcc_10-10.3.0-openmpi_4-4.0.7/1.12.0/lib:$LD_LIBRARY_PATH"

export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/u/adutt/ParaView-5.11.0-osmesa-MPI-Linux-Python3.9-x86_64/lib/catalyst:/u/adutt/ParaView-5.11.0-osmesa-MPI-Linux-Python3.9-x86_64/lib"

srun $PROG
